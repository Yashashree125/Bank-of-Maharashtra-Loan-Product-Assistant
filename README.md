# Bank-of-Maharashtra-Loan-Product-Assistant

An AI-powered assistant that answers questions about Bank of Maharashtra loan offerings using a Retrieval-Augmented Generation (RAG) pipeline. This project includes a polite web scraper, chunking pipeline, FAISS-based vector search, a Groq LLM for generation, and a FastAPI + HTML/CSS frontend.

## ğŸš€ Quick Start

### 1) Clone & create a virtual env

```bash
git clone <your-repo-url>
cd loan-assistant
python -m venv .venv
# Linux/Mac
source .venv/bin/activate
# Windows
.venv\Scripts\activate
```

### 2) Install dependencies

```bash
pip install -r requirements.txt
```

### 3) Configure environment

Create a **.env** file in the project root (do not commit this file):

```ini
GROQ_API_KEY=your_groq_api_key_here
MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
LLM_MODEL=openai/gpt-oss-120b
FAISS_DIR=output/faiss_index
```

### 4) Scrape pages (optional â€“ if you need fresh data)

Prepare `seed_urls.txt` (one URL per line), then run:

```bash
python scrappe.py
```

Outputs:

* `output/pages/` â€“ saved HTML snapshots
* `output/tables/` â€“ CSVs extracted from HTML tables
* `output/loan_data.jsonl` â€“ structured page data

### 5) Chunk the scraped text

```bash
python chunking.py
```

Outputs:

* `output/loan_chunks.jsonl`

### For Loan Product Assistant - Run the FastAPI app

```bash
uvicorn app.main:app --reload
```

Open: [http://127.0.0.1:8000](http://127.0.0.1:8000)

---


## ğŸ§  Architectural Decisions

### Why FastAPI + HTML/CSS?

* **FastAPI** is lightweight, fast, and easy to deploy; provides form submission and templating through Jinja2.
* **Server-side rendered HTML** keeps the stack simple for a POC and avoids CORS issues.

### RAG Design

* **Retriever**: FAISS over `sentence-transformers` embeddings for fast semantic search.
* **Augmentation**: Topâ€‘k chunks concatenated as context for the LLM.
* **Generator**: Groqâ€‘hosted LLM receives the user question + retrieved context.
* **Output Formatting**: LLM returns Markdown; we convert to HTML using `markdown2` and render via `{{ answer | safe }}`.

---

## ğŸ“š Libraries & Why

### Scraping

* `requests` â€“ robust HTTP client.
* `BeautifulSoup4` + `lxml` â€“ reliable parsing of HTML structure.
* `pandas.read_html` â€“ quick extraction of tabular data to CSV.

### Data Processing

* `re`, `json`, `hashlib` â€“ cleaning, serialization, deterministic IDs from URLs.

### RAG Pipeline

* `sentence-transformers` â€“ `all-MiniLM-L6-v2` for fast, competitive sentence embeddings.
* `faiss-cpu` â€“ highâ€‘performance vector search on CPU.
* `groq` â€“ call the Groq LLM (`openai/gpt-oss-120b`) for answer generation.

### App & UX

* `fastapi`, `uvicorn` â€“ API server & dev reloader.
* `jinja2` â€“ templating for the UI.
* `markdown2` â€“ render Markdown output from LLM to HTML for bold/italics/lists.
* `python-dotenv` â€“ load secrets/config from `.env`.

---

## ğŸ“‘ Data Strategy (Chunking)

* **Chunk size**: `400` words
* **Overlap**: `50` words

**Rationale**

* Loan documents span definitions, eligibility, and rate details across multiple sentences/sections. Overlap preserves continuity so critical terms arenâ€™t split across chunks.
* 400â€‘word chunks strike a balance between **retrieval precision** (narrow scope) and **context coverage** (enough surrounding info for the LLM).

---

## ğŸ¤– Model Selection

* **Embedding model**: `sentence-transformers/all-MiniLM-L6-v2`

  * Small, fast, and strong for semantic search in English; excellent for CPU inference at POC scale.
* **LLM**: `openai/gpt-oss-120b` via **Groq**

  * Deterministic settings (low temperature) for factual, concise answers constrained to the provided context.

**Prompting**

* System rules enforce: *use only context*, *be concise*, and *return Markdown for readability* (converted to HTML in the UI).

---

## ğŸ›  AI Tools Used

* **FAISS** â€“ vector search engine for chunk retrieval.
* **SentenceTransformers** â€“ generates embeddings for both queries and chunks.
* **Groq LLM** â€“ generates answers conditioned on retrieved context.
* **markdown2** â€“ converts Markdown to HTML for a clean UI.
* **FastAPI + Jinja2** â€“ backend + templated frontend.

---

## âš ï¸ Challenges & Solutions

1. **Dynamic/JS-heavy pages**: Focused on static loan/product pages; filtered links heuristically (`loan|interest|maha-super`).
2. **Messy table formats**: Used `pandas.read_html` and saved as CSV, preserving rate slabs and eligibility data for future structured use.
3. **Noisy HTML**: Stripped nonâ€‘content tags (script/style/nav/header/footer) and extracted only headings/paras/lists.
4. **Duplicate/nearâ€‘duplicate content**: Deterministic URL hashing for file names; simple visitedâ€‘set to avoid re-fetching.
5. **Answer Length**:Sometimes responses were verbose; solved by prompt engineering with instructions like â€œAnswer in concise formâ€.
6. **Response formatting**: LLM returned Markdown; rendered it as HTML via `markdown2` and `{{ answer | safe }}` to avoid raw `**` and `*` showing up.

---

## ğŸ”­ Potential Improvements

* **Add LangChain** to simplify RAG pipeline (retriever, chain management).
* **Freshness**: Scheduled scraping (cron) + incremental reâ€‘indexing.
* **Tableâ€‘aware retrieval**: Embed tables separately; optionally serialize as Markdown for better retrievability.
* **Hybrid search**: Combine keyword + vector search; add metadata filters (product type, date, source page).
* **Caching**: Cache embeddings and query results to reduce latency.
* **Conversation memory**: Maintain short session memory to support followâ€‘ups (without switching stacks).



